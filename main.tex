\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{todonotes}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Balanced Information Gathering and Goal-Oriented Actions in Shared Autonomy for Multitasking Teleoperators\\
%Blind review
%thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Blind Review Author 1}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Blind Review Author 2}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
Teleoperation of robotic system can be a complex task due to factors such as high degree-of-freedom manipulators, operator inexperience, and operator situational awareness. In order to address the complexity of this task, researchers have developed shared autonomy as a paradigm of control that involves the shared control of a robot by a human teleoperator and an autonomous control system. We introduce the concept of active learning into a shared autonomy control system. Our approach centers on applying the ideas of information gain through creating an autonomous system that applies information gathering actions: actions that move to robot positions that help the autonomous system minimize its uncertainty about possible operator goals based on the operator's next action. We create a system for interweaving these information gathering actions with goal-oriented actions. We conduct an evaluation for use cases involving multitasking or distracted operators that compares pure teleoperation with two forms of shared autonomy systems: our system and a state-of-the-art goal-oriented shared autonomy system. We note significant improvements for both shared autonomy systems over pure teleoperation, and we additionally find some trade-offs between information gathering and goal-oriented actions that could help inform the scenarios in which one strategy may be more advantageous than the other.
\end{abstract}

\section{Introduction}

Shared autonomy is a robotic control paradigm that provides a means for the advances in autonomous control systems to empower humans operating a robot through blending autonomous control with human operation control inputs \cite{dragan2012formalizing, dragan2013policy, gopinath2017human}. This control paradigm can be especially useful in cases in which the human operator is unable to provide sufficient control input, whether due to inexperience with the system, operator distraction, physical limitation, or some other factor. Shared autonomy enables robots to make use of both the information that is provided by a human operator and the robot's own internal control and perception systems. In order for these two sources of control to work together effectively, it is critical that they have the same goal.

\begin{figure}
\missingfigure[figwidth=1\columnwidth]{Teaser Photo}
\caption{A participant controls a robot during a multi-tasking user study}
\label{teaser}
\end{figure}

Previous work has suggested that one design goal in facilitating better human-robot interaction is to enable the robot to understand natural and implicit cues from humans rather than only supporting explicit communication \cite{goodrich2003seven}. In shared autonomy, robots might similarly learn about operator goals in an implicit manner by observing operator control actions. This supports more fluid implicit communication and avoids forcing the operator to explicitly designate their desired goal, which may be difficult due to the interaction paradigm being used (i.e., only a joystick), the goal states being hard to describe due to high degree-of-freedom state space, or other factors. Rather than requiring the operator to explicitly designate their desired goal, the robot in a shared autonomy system builds a belief model of possible goals by observing the control actions input by the operator at different states. Intuitively, this involves abstracting the operator's individual actions to see them as a part of a larger goal-oriented policy.

We want robots to be able to perform this observe-and-learn task as efficiently as possible. There are two fundamentally different ways of approaching this: first, adjusting the model of how the robot learns from what it happens to observe, and second, adjusting what the robot will observe in order to help it learn - i.e., \textit{active learning}. In this paper, we investigate active learning through information gathering within a shared autonomy paradigm.

This approach involves the robotic system manipulating the states, or robot positions, at which actions will be observed based on the amount of knowledge the robot expects to gain from different states. In doing so, the system hopes to gain information more efficiently through greater information gain from each operator action observed. The operator changes nothing about their behavior and still provides the same implicit cues, but information is gained more rapidly through exploiting areas where the inferred internal policy of the operator is maximally different for different candidate goals. We refer to this strategy of moving toward information-rich states as information gathering actions, which contrasts with taking actions that move toward what are believed to be the operator's current goals, what we call goal-oriented actions. 

In this paper, we create such a system and examine its effects in a user study involving shared autonomy control for a multitasking operator. Our contributions include creating a system for incorporating information gathering during shared autonomy, as well as evaluation of both this system and a goal-oriented shared autonomy system compared to pure teleoperation on a task involving a multi-tasking human operator.

\section{Background}

Our approach draws on prior work from a few different areas. We use research from goal-directed shared autonomy in developing the base of our system, then apply research on information gathering to create our active information gathering system.

\subsection{Goal Prediction in Shared Autonomy}

Prior research has investigated shared autonomy and assistive teleoperation in an effort to make robot teleoperation more efficient, easier, and safer.

Early work in this domain assumed that the robot was given information about the current goal or desired behavior and used this knowledge to determine assistance strategies \cite{aigner1997human, debus2001cooperative, goodrich2001experiments}. While systems relying on such knowledge can be efficient, we are interested in creating more fluid interaction that does not require explicit communication of operator goals.

Recent work in shared autonomy and assistive teleoperation has moved toward relaxing previously-held assumptions about what information the robot is given about the current task. Many of these more recent studies have demonstrated assistive teleoperation systems that are not given the user's goal ahead of time. Instead, these systems rely on first predicting the user intent, then using a distinct assistive strategy that assists toward the currently predicted user goal \cite{dragan2012formalizing, fagg2004extracting, kragic2005human, schultz2017goal, yu2005telemanipulation}. This ``predict-first'' framework has the benefit of providing assistance without a priori knowledge of the user's goal, but has a drawback in that the goal-oriented assistance is only available once the current goal has been predicted with sufficient confidence. 

An exciting recent avenue of study that our work builds upon and investigates for new use cases is the use of hindsight optimization to assist toward multiple goals simultaneously. \citet{javdani2015shared} set up a partially observable Markov decision process (POMDP) and use QMDP approximation \cite{littman1995learning}, or hindsight optimization, to estimate the best robot action at each timestep. This approximation eliminates information gathering from the POMDP solution and results in system action selection that weights the assistance toward each goal by the current belief in that goal. By selecting actions that optimize based on the current belief state over all goals, such a system enables goal-oriented assistance without a required confidence level for prediction of the operator's goal. This method was tested on two different shared control teleoperation tasks during which both operator and system-generated controls were passed to the robot, with results demonstrating significant improvements in efficiency for the hindsight optimization system over both the previously mentioned predict-first strategies and pure teleoperation \cite{javdani2018shared}. While this is a promising approach to goal-oriented assistance, it does not utilize the potential for shared autonomy systems to explore the state space in order to learn about the operator's goals.

\subsection{Active Information Gathering over Human State}
Our novel shared autonomy system makes use of exploration by integrating robotic actions that actively gather information about human objectives. We were inspired by the work of \citet{sadigh2016information} on building a system for an autonomous car that tests how human drivers react to certain ``probing" actions of the autonomous car, such as starting to merge in front of a human driver, in order to glean information about human driver's internal state (i.e., a driver's aggressiveness). Their approach centers on using a reward function that minimizes the entropy of the belief state about human drivers' internal states.

\citet{liu2017bignav} apply a similar strategy in creating a system for user view navigation that outperforms pan and zoom methods by displaying views to a user that maximize information gain. Their system uses the same principle of information gain to choose new views as a user navigates through a large multiscale interface in order to more quickly ascertain the user's ultimate intended view.

\subsection{Shared Autonomy for a Multi-Tasking Operator}
The context for which we develop and test our system is that involving an intermittently-absent human operator. One measure of the usefulness of a robot's autonomy is neglect tolerance, or the ability of human operators to neglect the robot \cite{crandall2002characterizing, olsen2003metrics}. The ability of the robot to carry on with its task while unsupervised allows human operators to attend to other tasks, such as operating other robots \cite{crandall2005validating, cummings2008predicting}.

In this work, we investigate how shared autonomy systems might support human operators in a task that requires human operators to only have intermittent control of the robot as they must perform other tasks simultaneously (such as monitoring and logging sensor readings). We believe such scenarios may represent opportune use cases for shared autonomy, where shared autonomy may increase the neglect tolerance of the robot, even without prior knowledge of the operator's exact goal.

\section{Information Gathering in Shared Autonomy}

In this section, we formally describe our formulation for active information gathering during shared autonomy.

\subsection{Formulation}

Let $ x \in X $ represent the system state. This could encompass just the robot joint position, a more involved model including physical constraints, or some other representation (in our implementation, state represented the position of the robot's end effector in 3D space and a momentum factor consisting of the currently active movement plan's direction). We assume that the operator in our teleoperation system has a unique goal in mind, $\phi \in \Phi$. Actions are represented by $ u \in U $, with $u_H$ denoting user-input actions. This includes both actions input by the user or actions actuated by the robot. Finally, we also assume access to a model of the system transition function, $T: X \times U \rightarrow X$.

\subsection{Operator Action Observation}
In order to use our observations of operator actions at each timestep $u_H^t$ to update our estimate of $\phi$, we estimate the model of the operator's reward function as parametrized by $\phi$ that best explains the observed operator inputs. Other related research has used the principles of maximum entropy \cite{ziebart2008maximum} to create a model over trajectory probability distributions \cite{dragan2012formalizing, javdani2015shared}, while our approach uses a model over action probability distributions inspired by the approach introduced by  \citet{ramachandran2007bayesian}. We apply this simplification to action space instead of trajectory space because operator input actions do not necessarily correspond to actions taken by the system, thus the state-operator action pairs are not equivalent to a trajectory. With this in mind, we assume that the human optimizes based on individual input actions rather than over trajectories, since they do not have full control over the trajectory. Additionally, we do not assume that the goal positions are static; if goals move during execution, models built using action probability distributions are not affected since each action is parameterized by the current position of all possible goals. This simplification also allows us to compute the normalizing factor for our distribution analytically, as the action space is small and discrete, compared to using a continuous trajectory space, which would require us to estimate the normalizing factor \cite{javdani2015shared}.

We define a reward function $R_\phi (x, u_H)$ that estimates the user's reward for choosing action $u_H$ in state $x$. This reward function could be defined based on important features such as distance to goal or could be learned from user observations. In this work, we use a reward function corresponding to the anticipated difference in distance to the goal before and after taking action $u_H$: 
\[
R_\phi (x, u_H) = dist(x, \phi) - dist(T(x, u_H), \phi)
\]
Like in \cite{ramachandran2007bayesian}, we use an exponential distribution over reward to model the operator's probability of choosing an action.
\[
P(u_H|\phi, x) \propto exp(R_\phi (x, u_H))
\]
Since our action space is discrete, we are able to normalize using a summation over possible actions.
\[
P(u_H|\phi, x) = \frac{exp(R_\phi (x, u_H))}{\sum_{u'} exp(R_\phi (x, u'))}
\]

Given an observation sequence of states and operator inputs, $\xi^T = \{(x^0, u_H^0), (x^1, u_H^1), ..., (x^T, u_H^T)\}$, we can then define the likelihood of this observation sequence for a given $\phi$ through the following equation:

\[
P(\xi|\phi) = \prod_tP(u_H^t|\phi, x^t)
\]

This assumes that the operator's policy is \textit{stationary}, i.e., it does not change through the course of the observation. Consequently, the individual action choices can be considered independent. We apply Bayes theorem to obtain our estimated likelihood of the goal given the current observations.

\[
P(\phi|\xi) = \frac{P(\xi|\phi)P(\phi)}{\sum_{\phi'}P(\xi|\phi')P(\phi')}
\]

Since our \textit{a priori} distribution of goal states is uniform, we can simplify to obtain our final goal likelihood equation:

\[
P(\phi|\xi) = \frac{P(\xi|\phi)}{\sum_{\phi'}P(\xi|\phi')}
\]

\subsection{Hindsight Optimization}

We incorporate hindsight optimization \cite{javdani2015shared} into our formulation by evaluating our reward function over a belief state. This gives us an estimate of the state-action value function that weights the reward for each state-action pair according to the current belief state. Using $b$ that encodes our belief state at time $t$:
\[
b_t(\phi) = P(\phi|\xi^t)
\]
Then, we apply the hindsight optimization approximation to estimate the value for each state-action pair:
\[
Q^{HO}(x,u,b_t) = \mathbb{E}_\phi[R_\phi (x, u)]
\]
\[
=\sum_\phi R_\phi (x, u_H)b_t(\phi)
\]

\subsection{Expected Information Gain at a New State}

In order to quantify the value of the information gain expected from a state, we estimate the reduction in entropy of the current belief state that will result from observing an operator action at that state. To do this, we use the Minimum Expected Entropy approach developed for active learning \cite{holub2008entropy}.

Let the entropy over the goal belief after an observation sequence $\xi$ be given as follows:

\[
H(\xi) = -\sum_\phi P(\phi|\xi)log_{|\Phi|}(P(\phi|\xi))
\]

Assume that a new state action pair is observed, $(x',u_H')$, creating observation sequence $\xi' = \xi \cup (x', u_H')$. The information gain $I$ from this new observation is the difference in entropy before and after the observation.

\[
I(\xi, \xi') = H(\xi) - H(\xi')
\]

However, the difficulty in evaluating the information gain of a state is that $u_H'$ is not known prior to visiting the state. To solve this, we follow the approach of \citet{holub2008entropy} by calculating the expected entropy through looping over all possible user actions at the state and summing each conditional entropy weighted by the likelihood of the user picking the action according to our current belief state about the user's goal, $b(\phi)$.

\[
\hat{H}(\xi, x') = \sum_u P(u|b(\phi),x')H(\xi \cup (x', u))
\]
\[
= \sum_u \sum_\phi P(u|\phi,x')P(\phi|\xi)H(\xi \cup (x', u))
\]

Using this expected entropy, we can calculate the expected information gain of going to a new state $x'$ without knowing which action the operator will input.

\[
\hat{I}(\xi, x') = H(\xi) - \sum_u \sum_\phi P(u|\phi,x')P(\phi|\xi)H(\xi \cup (x', u))
\]

\subsection{Incorporating User and Robot Actions}
While we can pick actions that maximize $\hat{I}(\xi, T(x,u))$ in order to most quickly ascertain the operator's goal, or actions that maximize $Q^{HO}$ in order to optimize assistance, these strategies may result in the system choosing actions that are dramatically opposed to the operator's inputs, potentially creating a frustrating operator experience. Accordingly, when choosing an action, we penalize actions that are far from those input by the operator (as measured by the distance between the control vectors for these two actions). To represent this penalty, we use the following term:

\[
D_{u_H}(u) = -||u - u_H||
\]

In our implementation, we did not apply this penalty when the user did not input an action (absent operator control).

\subsection{Action Selection}

We modulate the importance of information gathering actions and goal assistive actions by the current entropy in the system: if entropy is high, information gathering actions are more likely to be taken than goal assistive actions, and if entropy is low, goal assistive actions are more likely to be taken than information gathering actions. We accomplish this by weighting both $\hat{I}$ and $Q^{HO}$ by functions of our current entropy, $f_1$ and $f_2$ respectively. In our implementation, we use simple piecewise functions:
\[
f_1(H) = 
\begin{cases}
H & H\geq 0.1 \\
0.1 & H\leq 0.1
\end{cases}
,f_2(H) = \frac{1}{f_1(H)}
\]
This gives us three factors to determine the value of a candidate action: the expected information gain weighted by the current entropy, the movement toward the expected goal weighted by the inverse of the current entropy, and the distance between the candidate action and the operator input. We also add tuning parameters $\alpha$, $\beta$, and $\gamma$ to adjust the magnitude of each factor. Combining all the tuning parameters gives us our weight vector $\theta = [\alpha f_1(H), \beta f_2(H), \gamma]$ that we use in order to perform action selection.

\[
u^* = argmax_u [ \theta \cdot [\hat{I}(\xi, T(x,u)), Q^{HO}(x,u,b_t), D_{u_H}(u)]^T ]
\]

\subsection{Behavior}

These figures help visualize the differences in behavior between information gathering actions and hindsight optimization in a few different scenarios. (TODO: Can't really fill this out until making the figures).

\begin{figure}
\missingfigure[figwidth=1\columnwidth]{Cluttered Scene Behavior}
\caption{Heatmap of expected entropy gain by state (a) compared to heatmap of hindsight optimization reward (b) in which all goals have equal probabilities}
\label{heatmap1}
\end{figure}

\begin{figure}
\missingfigure[figwidth=1\columnwidth]{Cluttered Scene Behavior 2}
\caption{Heatmap of expected entropy gain by state (a) compared to heatmap of hindsight optimization reward (b) in which goals have different probabilities}
\label{heatmap2}
\end{figure}

\begin{figure}
\missingfigure[figwidth=1\columnwidth]{Simple Scene Behavior}
\caption{Heatmap of expected entropy gain by state (a) compared to heatmap of hindsight optimization reward (b) in simple scene containing only two goals}
\label{heatmap3}
\end{figure}

\section{User Study}

We conducted an experiment to investigate the differences between pure teleoperation, goal-oriented hindsight optimization shared autonomy, and active information gathering shared autonomy in operator multitasking scenarios.

\subsection{Implementation Details}\label{implementation}
In order to demonstrate our shared autonomy system and evaluate the effect of information gathering and hindsight optimization during operator multi-tasking, we created a system for controlling the end effector of a Fetch robot. We created this control system such that joystick controls on a Playstation 3 controller mapped to directions in Cartesian space, and two separate buttons could be pressed to open and close the end effector. Directional controls were discrete, with three possible controls for each axis (i.e., forward, no movement, and backward), and directional controls for each of the three axes could be combined to move diagonally. This created 27 possible directional control actions at each timestep. Absent user input and gripper controls were not used for belief updating; only directional controls were added as human control observations.

%Connor - for now I'm thinking we don't have room for a control diagram. If it looks like we do, we can use the one that was shown to participants

In this case, no direct manipulation of the pose of the end effector was involved in the control system, and all end effector movements followed Cartesian paths generated by MoveIt \cite{sucan2013moveit} with a max step size of 5cm between end effector configurations, creating smooth and linear movement of the end effector. We chose to create this simplified, linear control system instead of using direct manipulation over the robot's 7-DOF arm in order to make the controls quick to pickup and the robot's movement easy to understand for novice users. We made this choice because initial pretesting showed that novice users had significant trouble operating the joints directly. Our shared autonomy system used the same action set as the human operators.

Control updates occurred at a rate of 2 Hz to ensure that every action had visible effects, even after including the motion planning delay and robot joint acceleration. We also chose to prevent the robot from interrupting participants in this study, which was accomplished by setting an arbitrarily high value for $\gamma$, the tuning parameter that adjusts the acceptable distance between operator input and system output. While we believe that there are many interesting questions to address about interweaving these control systems with active human controls, we constrained the scope of our study to focus specifically on the impact of information gathering and hindsight optimization actions for multi-tasking operators through using these systems to take over during absent operator controls. We chose tuning values for $\alpha$ and $\beta$ (the tuning parameters for information gathering and hindsight optimizations actions, respectively) to keep the order of magnitude the same for the reward from information gathering actions and hindsight optimization actions, such that information gathering actions were weighted significantly higher than hindsight optimization actions with high entropy (e.g., $H > 0.7$), and hindsight optimization actions became dominant with low entropy (e.g., $H < 0.3$).

\subsection{Experiment Setup}

We ran a $3 \times 1$, within-participants user study ($N = 12$, 9 male and 3 female) that compared three different teleoperation control systems: pure teleoperation, hindsight optimization, and balanced information gathering. The control systems were setup as follows:

\subsubsection{Pure Teleoperation}
In this condition, the robot did not take any actions when the participant was not actively sending controls. Belief updating was still done for analysis purposes, but the belief state was not used in any way by the robot.
\subsubsection{Hindsight Optimization}
In this condition, $\alpha$ was set to 0, causing hindsight optimization to be the only factor when choosing actions during absent participant controls.
\subsubsection{Balanced Information Gathering}
In this condition, the full system was used for choosing actions including both hindsight optimization and information gathering factors.

Participants completed three trials, using a separate control system for each trial. Each trial consisted of participants operating the robot to move three of six objects from a table to two bins on the floor, as seen in Fig. \ref{exp_setup}. Participants were given ordered instructions by the experimenter of which object to grab and the bin to which that object should be moved. After each object was dropped into a bin, the arm reset itself to its starting position. This effectively created six tasks per trial - three ``pickup" tasks and three ``dropoff" tasks. The context participants were given was that of cleaning up after a ``chemical spill", requiring the use of the robot to dispose of these objects.

At the same time as participants were operating the robot, participants also were required to monitor a laptop approximately 5 feet away that would periodically receive new ``sensor readings". New sensor readings were signified by loud audio alerts. The alerts started 10 seconds into the trial, and then came every 20 seconds after that until the trial was completed. Participants were told they must log each sensor reading sometime within 20 seconds after each alert, before a new reading appeared. Logging the sensor readings required setting down the controller for the robot and turning away from the robot. This forced the participants to multitask between operating the robot with the controller and logging sensor readings on the laptop. This also forced regular interruptions of participant control inputs, during which time the different conditions affected how the robot behaved.

We took several steps to mitigate potential ordering effects that might arise due to our within-participants design. Our simplified linear robot control system (rather than full 7 DOF control) was designed to be simple enough that participants could rapidly become proficient, rather than learning and improving across trials. We also had a brief practice period before each trial during which participants operated the robot using the control system that they would be using in the upcoming trial. This practice period involved grabbing a single object (separate from those used for goals in the experiment) and moving it on the table, and ended once the participant was able to successfully grab, move, and drop the object (this typically lasted less than a minute). We also fully counterbalanced condition order to further mitigate any remaining ordering effects. Goals and ordering of goals for object pickup and dropoff were fully randomized, as were the values of the ``sensor readings" (although the timing for the sensor readings was constant for all trials). Between each trial, participants completed a survey over the control system that they just used.

%Connor - I do not have the exact values for the practice period length. We can cut that parenthetical phrase if the approximately looks bad

During the experiment, positions of all object goals and dropoff goals were provided to the robotic system by a VICON motion capture system. The system also switched between two different goal sets depending on the status of the gripper: when the gripper was open, the system used the positions of the objects to be picked up as candidate goals, and when the gripper was closed, the system used the positions of the dropff bins as candidate goals. Each time an object sorting task was completed, that object was removed from the system's list of candidate goals and the system's belief state and recorded human action observations were reset.

\subsection{Hypotheses}

Our hypotheses summarize our expectations of the differences between conditions.

TODO: hypotheses here

\begin{figure}
\missingfigure[figwidth=1\columnwidth]{Experiment Scene}
\caption{Experiment Setup}
\label{exp_setup}
\end{figure}

\section{Measures and Results}

During our experiments, we collected data about operator inputs, robot actions, and robot belief state in order to analyze the effects of our conditions.

\subsection{Measures}

To evaluate the impact of the control systems, we split the data from each trial into six separate tasks (three pickup and three dropoff) and looked at the belief in the \textit{correct} goal and the entropy at each timestep, as well as task time to completion. We also used the concept of ``shared autonomy control windows" during our evaluation - we considered a shared autonomy control window to be any sequence of timesteps in which the participant did not input controls for at least 5 continuous timesteps, or 2.5 seconds. As the shared autonomy system only affected the control of the physical robot while participants were not sending controls, these windows were the times during which the experimental conditions affected the robot's behavior. We chose a minimum of 5 continuous timesteps to use for analysis in order to ensure that extremely short windows during which the control system would not have sufficient time to significantly move the arm were not counted. 

We also looked at the number of user actions observed before the correct goal belief became the dominant belief (the largest of all goal beliefs for the remainder of the task), which we call the belief convergence speed. We observed nine total tasks (two pure teleoperation, three balanced information gathering, and four hindsight optimization gathering) which ended before the correct belief had become dominant; these trials were not included in the analysis.

Finally, we measured the number of participant actions input before task completion. This measure gave us an indication of the task efficiency provided by the different conditions.

Each participant completed six tasks per condition, for a total of 216 tasks (72 tasks per condition). We counted pickup tasks as a success if the participant was able to successfully grab and move an object, and we counted dropoff tasks as a success if the participant was able to move the object above the bin and drop it such that it fell either in the bin or within 10 cm of the edges of the bin. 

There was only one failure during a pickup task (balanced information gathering condition), which occurred when a participant knocked over a goal object (a Windex bottle) while attempting to grab it. This task data was still used for analysis as the arm still went through the full trajectory for grabbing the correct goal and only failed during the gripping movement. However, the dropoff task data that would have followed this pickup task was not used due to there being no dropoff task since the pickup failed. There were three total failures during dropoff tasks, two of which (one hindsight optimization and one balanced information gathering conditions) occurred with the same grasped object (again, the same Windex bottle) slipping out of the gripper during movement toward the bin, and one (balanced information gathering condition) which occurred due to the participant dropping the object at the wrong bin mistakenly. These three tasks were not included in the rest of the analysis as they did not include full trajectories of operator control toward the correct goal.

We also used surveys after each trial to measure subjective participant experience about each condition. To create this survey, we used modified System Usability Scale (SUS) and NASA TLX scales with 7-point Likert-style questionnaire items for consistency between scales.


\subsection{Objective Results}

We found differences between our conditions in aspects of robot behavior and task efficiency. These differences were found both in qualitative observations and further analysis of the data from our study.

\subsubsection{Observations}
Qualitatively, we found that the system's belief in the correct goal converged much more rapidly in both the hindsight optimization and balanced information gathering conditions than the pure teleoperation condition for all tasks. However, we noticed a large difference in the behavior of balanced information gathering between the pickup and dropoff conditions - it was evident that the balanced information gathering condition was much more successful at converging to the correct belief in pickup tasks than in dropoff tasks. Fig. \ref{belief_graphs} visualizes this observation.

\begin{figure}
\missingfigure[figwidth=1\columnwidth]{Belief Graphic}
\caption{These plots demonstrate the belief in the correct goal by number of user actions observed, averaged over all tasks within each condition. Plot (a) shows the averaged belief for all tasks (both pickup and dropoff), while Plot (b) shows the averaged belief for only the pickup tasks. These plots show that the balanced information gathering condition provided much more benefit during pickup tasks, when the number of candidate goals was higher}
\label{belief_graphs}
\end{figure}

\subsubsection{Analysis}

We analyzed this data using one-way analysis of variance (ANOVA) tests with the experimental condition as a fixed effect and the condition order and goal for each task as random covariates. To better character system performance across condition, we analyzed performance both across just the pickup tasks as well as across all tasks (pickup and dropoff). We did this because we noticed a difference in the impact of the balanced information gathering condition between pickup and dropoff tasks during qualitative observations, with larger effects being seen for pickup tasks. Finally, we analyzed the results from the two scales within our participant survey separately using one-way analysis of variance (ANOVA) tests with the experimental condition as a fixed effect.

\subsubsection{Efficiency of Belief Gain after Shared Autonomy Control}
To test the effects of shared autonomy control windows, we scored each control window as the improvement in the goal belief that occurred from the first action once the participant resumed control. This demonstrates how the robot's behavior during the shared autonomy control window influenced the information that the system gained once the participant returned. 

%removing because probably unneeded
%First, we looked at the belief improvement after the first shared autonomy control window during each task to evaluate the impact of the shared autonomy systems at the beginning of a task. For all tasks, we found a significant effect of condition on belief improvement, $F(2, 186.5) = 5.66$, $p < .01$. Post-hoc comparisons using Tukey's HSD test revealed a significant difference between balanced information gathering and pure teleoperation ($p < .01$), a marginally significant difference between hindsight optimization and pure teleoperation ($p = .070$), and no significant difference between balanced information gathering and hindsight optimization. For pickup tasks alone, we also found a significant effect of condition on belief improvement, $F(2, 91.31) = 12.06$, $p < .0001$. Post-hoc comparisons using Tukey's HSD test revealed a significant difference of balanced information gathering over both pure teleoperation ($p < .0001$) and hindsight optimization ($p < .001$), with no significant difference between hindsight optimization and pure teleoperation.

We looked at all shared autonomy control windows during which the entropy was still higher than a threshold of 0.7. We chose this threshold due to this being the approximate value above which information gathering actions were dominant over hindsight optimization actions based on our system tuning, as described in \S\ref{implementation}. For all tasks, we found a significant effect of condition on belief improvement, $F(2, 243.5) = 7.72$, $p < .001$. Post-hoc comparisons using Tukey's HSD test revealed that both balanced information gathering ($p < .001$) and hindsight optimization ($p < .05$) significantly outperformed pure teleoperation in terms of belief improvement (TODO: list of M, SD for all conditions needed). We did not find a significant difference between balanced information gathering and hindsight optimization. For pickup tasks alone, we also found a significant effect of condition on belief improvement, $F(2, 104.7) = 16.57)$, $p < .0001$, with Tukey's HSD test showing significant differences of balanced information gathering over both pure teleoperation ($p < .0001$) and hindsight optimization ($p < .0001$), and no significant difference between hindsight optimization and pure teleoperation. These results are shown in Fig. \ref{picture_results} (a).

\subsubsection{Belief Convergence Speed}

For all tasks, we found a significant effect of condition on speed of gaining the proper belief, $F(2, 173.7) = 13.22$, $p < .0001$. Post-hoc comparisons using Tukey's HSD test revealed significant differences between balanced information gathering and pure teleoperation ($p < .01$) and between hindsight optimization and pure teleoperation ($p < .0001$), but no significant difference between balanced information gathering and hindsight optimization. For pickup tasks alone, we found a significant effect of condition on speed of gaining the proper belief, $F(2, 87.15) = 9.52$, $p < .001$. Post-hoc comparisons using Tukey's HSD test again revealed significant differences between balanced information gathering and pure teleoperation ($p < .001$) and between hindsight optimization and pure teleoperation ($p < .01$), but no significant difference between balanced information gathering and hindsight optimization. Fig. \ref{picture_results} (b) visualizes these results.

\subsubsection{Task Efficiency}

Finally, we the task efficiency of the participants while using each control system. For all tasks, we found a significant effect of condition on task efficiency, $F(2, 181.1) = 49.31$, $p < .0001$. Post-hoc comparisons using Tukey's HSD test revealed significant differences between all pairwise conditions ($p < .0001$), with hindsight optimization being the most efficient, followed by balanced information gathering, and pure teleoperation requiring the most participant input. We found similar results for pickup tasks alone, with condition having a significant effect on task efficiency $F(2, 87.8) = 27.81$, $p < .0001$. However, for these tasks, balanced information gathering and hindsight optimization both were significantly different from pure teleoperation ($p < .0001$), but not significantly different from each other ($p = 0.134$). The results from these tests can be seen in Fig. \ref{picture_results} (c)

\begin{figure*}
\missingfigure[figwidth=\textwidth]{ANOVA Test Results}
\caption{This figure shows the results of the ANOVA tests on our results. Part (a) contains the tests on efficiency of belief gain after shared autonomy control, Part (b) contains the tests on speed of gaining dominant proper belief, Part (c) contains the tests on task efficiency, and Part (d) contains results from our subjective measures}
\label{picture_results}
\end{figure*}

\subsection{Subjective Results}

We found a significant effect of condition on system usability, $F(2, 22) = 8.21$, $p < .01$, with participants rating the balanced information gathering system significantly less usable than both the hindsight optimization system ($p < .05$) and pure teleoperation ($p < .01$), but no significant difference between hindsight optimization and pure teleoperation. Similarly, we found a significant effect of condition on perceived workload, $F(2, 22) = 5.60$, $p < .05$, with participants rating the balanced information gathering condition as having a higher workload than both other conditions ($p < .05$). These effects are shown in Fig. \ref{picture_results} (d).

\section{Discussion}
Our results demonstrate several trade-offs between information gathering and hindsight optimization control systems. While we found that information gathering actions did lead to more efficient belief updating, hindsight optimization control systems more rapidly reached the operator's goals. The lack of a significant difference in time to reach the correct dominant belief despite the difference in the efficiency of belief updating suggests that there were not enough windows of shared control for the performances of the two systems to diverge significantly, perhaps showing that a longer or more involved task might benefit more from information gathering. Additionally, we noticed a much larger improvement in belief updating for information gathering systems over hindsight optimiziation systems during our pickup tasks than during our dropoff tasks; we speculate that this is due to the more complex scene with an increased number of potential goals, but further research is necessary to investigate how balanced information gathering and hindsight optimization scale across scene complexity.

Importantly, both systems perform better than pure teleoperation at both speed of gaining the correct dominant belief and task efficiency. This demonstrates that while there is a trade-off between these two shared autonomy systems, neither system results in a loss of performance from pure teleoperation in any of our measured areas.

The finding that participants rank the information gathering system as less usable and causing a higher workload backs up prior ideas that humans are resistant to being used as an "oracle" \cite{javdani2015shared}. However, recent work in estimating operator adaptability \cite{nikolaidis2017human} could be applied to weighting the amount of information gathering actions dynamically for each user, fitting with other research that has found that shared autonomy systems perform best when strength of assistance is customized per user \cite{gopinath2017human}. Additionally, it is possible that further understanding of the system would mitigate the confusion caused by novice users not understanding the robot's information gathering motions, as these were not always goal-oriented actions. The lack of a significant difference between hindsight optimization and pure teleoperation demonstrates that multi-tasking scenarios are a good fit for hindsight optimization control systems, as previous studies with interwoven user and shared autonomy controls showed users tended to prefer pure teleoperation over hindsight optimization \cite{javdani2018shared}.

Our demonstrated effects of the impact of information gathering actions show that active learning can successfully be applied to teleoperation of robots. We believe that this is not a solution for all shared autonomy situations, but specifically fits well with scenarios in which gaining the correct belief about internal operator goal is of increased importance. This includes scenarios in which there are penalties for moving toward the wrong goal (e.g., guiding an aircraft toward one of many landing spots would require regaining altitude and circling if the wrong spot was targeted) or there is a danger of operator signal being dropped (e.g., remote control of search and rescue vehicles in a heavy signal-loss environment).

\section{Conclusion}

Shared autonomy provides a means to augment human teleoperator's actions with those of a robotic system. Previous work has investigated ways to choose goal-oriented assistive actions, even without a priori knowledge of operator goals. We have investigated the use of active information gathering actions within a shared autonomy paradigm in order to more quickly learn from the operator's actions about their intended goal.

The improvement that we found of both balanced information gathering and hindsight optimization systems over pure teleoperation on both belief gain efficiency and task efficiency demonstrate the power of shared autonomy for a multi-tasking operator. We have found some noteable trade-offs for picking between information gathering or purely goal oriented (hindsight optimization) systems, and we've discussed some areas for potential further work in examining how the performance of these two systems compare as number of candidate goals or task complexity increases.

We have designed a shared autonomy system for integrating information gathering actions and have shown both the improvements and trade-offs provided by information gathering compared to hindsight optimization during operator multi-tasking. Our data gives some insight into what kinds of scenarios will benefit from incorporating information gathering into shared autonomy, and we look forward to further investigation of these active learning shared autonomy systems.  

\bibliographystyle{plainnat}
\bibliography{sample}

\end{document}