\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Information Gathering and Hindsight Optimization in Shared Autonomy\\
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Connor Brooks}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Daniel Szafir}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
Abstract.
\end{abstract}

\section{Introduction}

Story: Good shared autonomy hinges on knowing what operator wants

Citations about users not having to directly select goal

Teaser photo here?

\section{Background}
\subsection{Goal Prediction in Shared Autonomy}

Much work has been done on investigating shared autonomy and assistive teleoperation in an effort to make robot teleoperation more efficient, easier, or safer. Recent work in shared autonomy and assistive teleoperation has moved toward relaxing previously-held assumptions about what information the robot is given about the current task. 

Early noteworthy work in this domain assumes that the robot has been given information about the current goal or desired behavior and uses this knowledge to determine assistance strategies \cite{aigner1997human, debus2001cooperative, goodrich2001experiments}. While systems relying on such knowledge can be efficient, this information may not be available in all cases.

Since these foundational works, many studies have demonstrated assistive teleoperation systems that are not given the user's goal ahead of time. Instead, these systems rely on first predicting the user intent, then using a distinct assistive strategy that assists toward the currently predicted user goal \cite{dragan2012formalizing, fagg2004extracting, kragic2005human, schultz2017goal, yu2005telemanipulation}. This framework has the benefit of providing assistance without knowledge of the user's goal, but has a drawback in that the assistance is only available once the current goal has been predicted with sufficient confidence. 

An exciting recent avenue of study that our work builds upon and investigates for new use cases is the use of hindsight optimization to assist toward multiple goals simultaneously. \citet{javdani2015shared} setup a partially observable Markov decision process (POMDP) and use QMDP approximation \cite{littman1995learning}, or hindsight optimization, to estimate the best action at each timestep. This approximation eliminates information gathering and results in action selection that weights the assistance toward each goal by the current belief in that goal. By selecting actions that optimize based on the current belief state over all goals, such a system enables assistance without a required confidence level for prediction of the user's goal. This method was tested on two different shared control teleoperation tasks during which both user and system-generated controls were passed to the robot, with results demonstrating significant improvements in efficiency for the hindsight optimization system over both the previously mentioned "predict-first" strategies and pure teleoperation \cite{javdani2018shared}.

\subsection{Active Information Gathering over Human State}
Our novel shared autonomy system integrates robotic actions that actively gather information about human objectives. We were inspired by the work of \citet{sadigh2016information} on building a system for an autonomous car that tests how human drivers react to certain "probing" actions of the autonomous car in order to glean information about the human drivers. Their approach centers on using a reward function that minimizes the entropy of the belief state about human drivers' internal states.

\citet{liu2017bignav} apply a similar strategy to a different domain, create a system for user view navigation that outperforms pan and zoom methods by displaying views to a user that maximize information gain. Their system uses the same principle of information gain to choose new views as a user navigates through a large multiscale interface in order to more quickly ascertain the user's ultimate intended view.

\subsection{Shared Autonomy for a Multi-Tasking Operator}
One measure of the usefulness of a robot's autonomy is the ability of human operators to neglect the robot \cite{crandall2002characterizing, olsen2003metrics}. The ability of the robot to carry on with its task while neglected allows human operators to attend to other tasks, such as operating other robots \cite{crandall2005validating, cummings2008predicting}.

In our study, we apply two shared autonomy systems to a task that requires human operators to only have intermittent control of the robot as they must perform other tasks simultaneously. This setup is intended to demonstrate the potential of shared autonomy to increase the neglect tolerance of the robot, even without prior knowledge of the operator's exact goal.

Our contributions include creating a system for incorporating information gathering during shared autonomy, as well as evaluation of both this system and a hindsight-optimization based system compared to pure teleoperation on a task involving a multi-tasking human operator.

\section{Information Gathering in Shared Autonomy}
\subsection{Formulation}

For our setup, let $ x \in X $ represent the system state. This could encompass just the robot joint position, a more involved model including physical constraints, or some other representation. We assume that the operator in our teleoperation system has a unique goal in mind, $\phi \in \Phi$. Actions taken in the system are represented by $ u \in U $, with $u_H$ denoting user-input actions. Finally, we also assume access to a model of the environment transition function, $T: X \times U \rightarrow X$.

\subsection{Operator Action Observation}
In order to use our observations of operator actions at each timestep $u_H^t$ to update our estimate of $\phi$, we estimate the model of the operator's reward function as parametrized by $\phi$ that best explains the observed operator inputs. Other related works \cite{dragan2012formalizing, javdani2015shared} have used the principles of maximum entropy \cite{ziebart2008maximum} to create a model over trajectory probability distributions, while our approach uses a model over action probability distributions instead such as that used by \citet{ramachandran2007bayesian}. We apply this simplification to action space instead of trajectory space for the reason that the operator input action does not necessarily correspond to the action taken by the system, so the state-operator action pairs are not equivalent to a trajectory. Due to this, we assume that the human optimizes based on individual input actions rather than over trajectories, since they do not have full control over the trajectory. Additionally, we do not assume that the goal positions are static; if goals move during execution, models built using action probability distributions are not affected since each action is parameterized by the current position of all possible goals. This simplification also allows us to compute the normalizing factor for our distribution analytically, as the action space is small and discrete while the trajectory space is continuous, creating difficulties computing the normalizing factor that may require estimations \cite{javdani2015shared}.

We define a reward function $R_\phi (x, u_H)$ that estimates the user's reward for choosing action $u_H$ in state $x$. This reward function could be defined based on important features such as distance to goal or could be learned from user observations. In this work, we use a reward function corresponding to the anticipated difference in distance to the goal before and after taking action $u_H$: 
\[
R_\phi (x, u_H) = dist(x, \phi) - dist(T(x, u_H), \phi)
\]
Like in \cite{ramachandran2007bayesian}, we use an exponential distribution over reward to model the operator's probability of choosing an action.
\[
P(u_H|\phi, x) \propto exp(R_\phi (x, u_H))
\]
Since our action space is discrete, we are able to normalize using a summation over possible actions.
\[
P(u_H|\phi, x) = \frac{exp(R_\phi (x, u_H))}{\sum_{u'} exp(R_\phi (x, u'))}
\]

Given an observation sequence of states and operator inputs, $\xi^T = \{x^0, u_H^0, x^1, u_H^1, ..., x^T, u_H^T\}$, we can then define the likelihood of this observation sequence for a given $\phi$ through the following equation:

\[
P(\xi|\phi) = \prod_tP(u_H^t|\phi, x^t)
\]

This assumes that the operator's policy is \textit{stationary}, i.e., it does not change through the course of the observation. Consequently, the individual action choices can be considered independent. We apply Bayes theorem to obtain our estimated likelihood of the goal given the current observations.

\[
P(\phi|\xi) = \frac{P(\xi|\phi)P(\phi)}{\sum_{\phi'}P(\xi|\phi')P(\phi')}
\]

Since our \textit{a priori} distribution of goal states is uniform, we can simplify to obtain our final goal likelihood equation:

\[
P(\phi|\xi) = \frac{P(\xi|\phi)}{\sum_{\phi'}P(\xi|\phi')}
\]

\subsection{Hindsight Optimization}

We incorporate hindsight optimization \cite{javdani2015shared} into our formulation by evaluating our reward function over a belief state. This gives us an estimate of the state-action value function that weights the reward for each state-action pair according to the current belief state. Using $b$ that encodes our belief state at time $t$:
\[
b_t(\phi) = P(\phi|\xi^t)
\]
Then, we apply the hindsight optimization approximation to estimate the value for each state-action pair:
\[
Q^{HO}(x,u,b_t) = \mathbb{E}_\phi[R_\phi (x, u)]
\]
\[
=\sum_\phi R_\phi (x, u_H)b_t(\phi)
\]

\subsection{Expected Information Gain at a New State}

In order to quantify the value of the information gain expected from a state, we estimate the reduction in entropy of the current belief state that will result from observing an operator action at that state. To do this, we use the Minimum Expected Entropy approach developed for active learning \cite{holub2008entropy}.

Let the entropy over the goal belief after an observation sequence $\xi$ be given as follows:

\[
H(\xi) = -\sum_\phi P(\phi|\xi)ln(P(\phi|\xi))
\]

Assume that a new state action pair is observed, $(x',u_H')$, creating observation sequence $\xi' = \xi \cup (x', u_H')$. The information gain $I$ from this new observation is the difference in entropy before and after the observation.

\[
I(\xi, \xi') = H(\xi) - H(\xi')
\]

However, the difficulty in evaluating the information gain of a state is that $u_H'$ is not known prior to visiting the state. To solve this, we follow the approach of \citet{holub2008entropy} by calculating the expected entropy through looping over all possible user actions at the state and summing each conditional entropy weighted by the likelihood of the user picking the action according to our current belief state about the user's goal, $b(\phi)$.

\[
\hat{H}(\xi, x') = \sum_u P(u|b(\phi),x')H(\xi \cup (x', u))
\]
\[
= \sum_u \sum_\phi P(u|\phi,x')P(\phi|\xi)H(\xi \cup (x', u))
\]

Using this expected entropy, we can calculate the expected information gain of going to a new state $x'$ without knowing which action the operator will input.

\[
\hat{I}(\xi, x') = H(\xi) - \sum_u \sum_\phi P(u|\phi,x')P(\phi|\xi)H(\xi \cup (x', u))
\]

\subsection{Incorporating User and Robot Actions}
While we can pick actions that maximize $\hat{I}(\xi, T(x,u))$ in order to most quickly ascertain the operator's goal, or actions that maximize $Q^{HO}$ in order to optimize assistance, these strategies may result in the system choosing actions that are dramatically opposed to the operator's inputs, creating a frustrating operator experience. Accordingly, when choosing an action, we penalize actions that are far from those input by the operator (as measured by the distance between the control vectors). To represent this penalty, we use the following term:

\[
D_{u_H}(u) = -||u - u_H||
\]


\subsection{Action Selection}

We modulate the importance of information gathering actions and goal assistive actions by the current entropy in the system: if entropy is high, information gathering actions are more likely to be taken than goal assistive actions, and if entropy is low, goal assistive actions are more likely to be taken than information gathering actions. We accomplish this by weighting both $\hat{I}$ and $Q^{HO}$ by functions of our current entropy, $f_1$ and $f_2$ respectively. In our experiments, we use simple piecewise functions:
\[
f_1(H) = 
\begin{cases}
H & H\geq 0.1 \\
0.1 & H\leq 0.1
\end{cases}
,f_2(H) = \frac{1}{f_1(H)}
\]
This gives us three factors to determine the value of a candidate action: the expected information gain weighted by the current entropy, the movement toward the expected goal weighted by the inverse of the current entropy, and the distance between the candidate action and the operator input. We also add tuning parameters $\alpha$, $\beta$, and $\gamma$ to adjust the magnitude of each factor. Combining all the tuning parameters gives us our weight vector $\theta = [\alpha f_1(H), \beta f_2(H), \gamma]$ that we use in order to perform action selection.

\[
u^* = argmax_u [ \theta \cdot [\hat{I}(\xi, T(x,u)), Q^{HO}(x,u,b_t), D_{u_H}(u)]^T ]
\]

\subsection{Behavior}

Some limited analysis of behavior. Heatmap of entropy-minimizing states in 2D. Examples where radically different from hindsight optimization (moving toward two points in a line, for example)
2-3 Figures here

\section{User Study}

\subsection{Implementation Details}
In order to demonstrate our shared autonomy system and evaluate the effect of information gathering and hindsight optimization during operator multi-tasking, we created a system for controlling the end effector of a Fetch robot in 3D space. We created this control system such that joystick controls mapped to directions in Cartesian space and two separate buttons could be pressed to open and close the end effector. Directional controls were discrete, with three possible controls for each axis (i.e., forward, no movement, and backward), and directional controls for each of the three axes could be combined to move diagonally. This created 27 possible directional control actions at each timestep. 

In this case, no direct manipulation of the pose of the end effector was involved in the control system, and all end effector movements followed Cartesian paths generated by MoveIt (CITATION NEEDED) with a max step size of 5cm between end effector configurations, creating smooth and linear movement of the end effector. We chose to create this simplified, linear control system instead of using direct manipulation over the robot's 7-DOF arm in order to make the controls quick to pickup and the robot's movement easy to understand for novice users.

Our shared autonomy syste


During the experiment, positions of all object goals and dropoff goals were provided to the robotic system by a VICON motion capture system. Once an object moving task was completed, that object was removed from the system's list of candidate goals.

Tuning choices (no interruptions), Controls update rate (and why so slow - planning time accommodation), etc..

\subsection{Experiment Setup}

We ran a $3 \times 1$, within-participants user study that compared three different teleoperation control systems: pure teleoperation, hindsight optimization, and information gathering. Participants completed three trials, using a separate control system for each trial. Each trial consisted of participants operating the robot to move three of six objects from a table to two bins on the floor. Participants were given instructions of which object to grab and the bin to which that object should be moved one at a time by the experimenter. At the same time, participants were told they must monitor a laptop that would periodically receive new "sensor reading" alerts and log these readings. The alerts started 10 seconds into the trial, and then came every 20 seconds after that until the trial was completed. Participants were told they must log each sensor reading sometime within the 20 seconds before a new reading appeared. Logging the sensor readings required setting down the controller for the robot and turning away from the robot. This forced the participants to multi-task between operating the robot with the controller and logging sensor readings on the laptop. This also forced regular interruptions of participant control inputs, during which time the different conditions affected how the robot behaved.

Although we attempted to minimize ordering effects by creating a simplified control system for the robot, we also fully counterbalanced condition order to further mitigate any remaining effects. Additionally, participants were given a brief practice period before each trial during which they operated the robot using the control system that they would be using in the upcoming trial. This practice period involved grabbing a single object (separate from those used for goals in the experiment) and moving it on the table, and ended once the participant was able to successfully grab, move, and drop the object - this typically lasted less than a minute. Goals and ordering of goals for object pickup and dropoff were fully randomized, as were the values of the "sensor readings" (although the timing for the sensor readings was constant for all trials).
\section{Results and Discussion}
2-3 Figures here

\section{Future Work}

\bibliographystyle{plainnat}
\bibliography{sample}

\end{document}